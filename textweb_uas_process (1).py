# -*- coding: utf-8 -*-
"""TEXTWEB UAS process.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zhuUASoxCZli9wxaNKPb6q3DI5fWiKWe
"""

import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
from difflib import get_close_matches
import pickle
from sklearn.cluster import KMeans
import numpy as np
from sklearn.neighbors import NearestNeighbors
import requests
from bs4 import BeautifulSoup
import time

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/131.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "id-ID,id;q=0.9,en-US;q=0.8,en;q=0.7",
}
BASE_URL = "https://cookpad.com"
REQUEST_DELAY = 1.5     # jeda antar request
TARGET_RECIPES = 50   # target jumlah resep


def get_soup(url, sleep_time=REQUEST_DELAY):
    """Download HTML dan ubah jadi BeautifulSoup."""
    print(f"[GET] {url}")
    resp = requests.get(url, headers=HEADERS, timeout=20)
    resp.raise_for_status()
    time.sleep(sleep_time)
    return BeautifulSoup(resp.text, "lxml")


def safe_get_text(tag, default=""):
    return tag.get_text(" ", strip=True) if tag else default


def normalize_ingredients_list(ingredients_list):
    """
    Bersihin list bahan:
    - buang baris judul: 'Bahan utama', 'Bumbu', dll
    - hapus angka & satuan di depan ('500 gr', '1/2 papan', dll)
    - hilangkan 'secukupnya'
    - kembalikan: 'daging ayam cincang, telur, wortel, bihun, ...'
    """
    cleaned = []
    for item in ingredients_list:
        text = item.lower().strip()

        # skip header section seperti "bahan utama", "bumbu"
        if re.match(r'^(bahan|bumbu)\b', text) and len(text.split()) <= 3:
            continue

        # buang isi dalam kurung
        text = re.sub(r'\(.*?\)', '', text)

        # hapus jumlah + satuan di depan, misal "500 gr", "1/2 papan"
        text = re.sub(r'^[0-9¼½¾⅓⅔/.\s]+[a-zA-Z]*\s*', '', text)

        # hapus kata satuan di depan kalau masih ada
        text = re.sub(
            r'^(sdt|sdm|sendok makan|sendok teh|gram|gr|kg|ml|liter|butir|buah|siung|lembar|batang|papan|bungkus|sachet)\s+',
            '',
            text,
        )

        # hapus 'secukupnya'
        text = re.sub(r'\bsecukupnya\b', '', text)

        # sisakan huruf/angka/spasi
        text = re.sub(r'[^a-z0-9\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()

        if not text:
            continue
        if text not in cleaned:
            cleaned.append(text)

    return ", ".join(cleaned)

def clean_title(title: str) -> str:
    if not title:
        return ""
    return title.lower().strip()


def infer_category(title_cleaned: str, ingredients_cleaned: str) -> str:
    """
    Infer kategori simpel berdasarkan kata kunci di judul + bahan.
    """
    text = f"{title_cleaned} {ingredients_cleaned}"

    rules = [
        ("ayam", "ayam"),
        ("ikan", "ikan"),
        ("daging sapi", "daging sapi"),
        ("sapi", "daging sapi"),
        ("kambing", "kambing"),
        ("daging", "daging"),
        ("telur", "telur"),
        ("tahu", "tahu"),
        ("tempe", "tempe"),
        ("nasi", "nasi"),
        ("mie", "mie"),
        (" mi ", "mie"),
        ("bihun", "bihun"),
        ("kue", "kue"),
        ("cake", "kue"),
        ("roti", "roti"),
        ("dessert", "dessert"),
        ("puding", "dessert"),
        ("sup", "sup"),
        ("soup", "sup"),
        ("sayur", "sayur"),
        ("sambal", "sambal"),
        ("bakso", "bakso"),
        ("soto", "soto"),
    ]

    for kw, cat in rules:
        if kw in text:
            return cat

    return "lainnya"


#PARSE HALAMAN DETAIL RESEP

def extract_recipe_cookpad(recipe_url: str) -> dict:
    """
    Ambil data 1 resep Cookpad:
    - Title, Ingredients, Steps, URL
    - Turunan: Category, Title Cleaned, Ingredients Cleaned, Total Ingredients, Total Steps
    """
    soup = get_soup(recipe_url)

    # 1) Judul
    title_tag = soup.find("h1")
    title = safe_get_text(title_tag)

    # 2) Bahan-bahan
    ingredients_list = []
    h_bahan = soup.find(
        lambda tag: tag.name in ["h2", "h3"]
        and "bahan-bahan" in tag.get_text(strip=True).lower()
    )
    if h_bahan:
        ul = h_bahan.find_next(["ul", "ol"])
        if ul:
            for li in ul.find_all("li", recursive=True):
                txt = safe_get_text(li)
                if txt:
                    ingredients_list.append(txt)

    # 3) Cara Membuat
    steps_list = []
    h_steps = soup.find(
        lambda tag: tag.name in ["h2", "h3"]
        and "cara membuat" in tag.get_text(strip=True).lower()
    )
    if h_steps:
        ol = h_steps.find_next(["ol", "ul"])
        if ol:
            for li in ol.find_all("li", recursive=True):
                txt = safe_get_text(li)
                if txt:
                    steps_list.append(txt)

    # gabung jadi string mentah
    ingredients_raw = " -- ".join(ingredients_list)
    steps_raw = " ".join(steps_list)

    # kolom turunan
    title_cleaned = clean_title(title)
    ingredients_cleaned = normalize_ingredients_list(ingredients_list)
    total_ingredients = len(ingredients_list)
    total_steps = len(steps_list)
    category = infer_category(title_cleaned, ingredients_cleaned)

    return {
        "Title": title,
        "Ingredients": ingredients_raw,
        "Steps": steps_raw,
        "URL": recipe_url,
        "Category": category,
        "Title Cleaned": title_cleaned,
        "Total Ingredients": total_ingredients,
        "Ingredients Cleaned": ingredients_cleaned,
        "Total Steps": total_steps,
    }


#SCRAPE HALAMAN PENCARIAN COOKPAD

def scrape_cookpad_search(keywords, max_pages_per_keyword=20, target_recipes=TARGET_RECIPES):
    """
    Ambil banyak resep dari Cookpad lewat halaman pencarian.
    keywords: list kata kunci (ayam, ikan, dll)
    max_pages_per_keyword: halaman maksimal per keyword
    """
    all_data = []
    visited_urls = set()

    for kw in keywords:
        print(f"\n=== SCRAPING KEYWORD: {kw} ===")
        for page in range(1, max_pages_per_keyword + 1):
            search_url = f"{BASE_URL}/id/cari/{kw}?page={page}"
            try:
                soup = get_soup(search_url)
            except Exception as e:
                print(f"[Search {kw}] gagal ambil page {page}: {e}")
                break

            # link-link resep
            recipe_links = soup.select("a[href*='/id/resep/']")
            if not recipe_links:
                print(f"[Search {kw}] tidak ada link resep di page {page}, stop keyword ini.")
                break

            for a in recipe_links:
                href = a.get("href")
                if not href:
                    continue

                # pastikan URL lengkap
                if href.startswith("http"):
                    recipe_url = href
                else:
                    recipe_url = BASE_URL + href

                # filter biar benar-benar halaman resep
                if "/id/resep/" not in recipe_url:
                    continue

                # buang parameter di akhir
                recipe_url = recipe_url.split("?")[0]

                if recipe_url in visited_urls:
                    continue

                visited_urls.add(recipe_url)

                try:
                    data = extract_recipe_cookpad(recipe_url)
                    # minimal punya judul & bahan
                    if data["Title"] and data["Ingredients"]:
                        all_data.append(data)
                        print(f"[TOTAL] {len(all_data)} resep")

                        if len(all_data) >= target_recipes:
                            print("Target resep tercapai.")
                            return all_data

                except Exception as e:
                    print(f"[Detail] gagal ekstrak {recipe_url}: {e}")

    return all_data


# main

def main():
    # kata kunci bisa kamu tambah sendiri biar variasi
    keywords = [
        "ayam", "ikan", "daging", "telur", "tahu", "tempe",
        "sayur", "sambal", "nasi", "mie", "kue", "dessert",
        "sup", "tumis", "goreng", "bakso", "soto"
    ]

    try:
        recipes = scrape_cookpad_search(
            keywords=keywords,
            max_pages_per_keyword=30,    # bisa dinaikkan kalau masih kurang
            target_recipes=TARGET_RECIPES
        )
    except KeyboardInterrupt:
        print("Dihentikan manual, menyimpan data sementara...\n")
        recipes = recipes if 'recipes' in locals() else []

    df = pd.DataFrame(recipes)
    if df.empty:
        print("Tidak ada data berhasil di-scrape.")
        return

    # buang duplikat
    df = df.drop_duplicates(subset=["URL"]).reset_index(drop=True)

    print("Total resep setelah buang duplikat:", len(df))

    output_path = "my_food_recipes_new.csv"
    df.to_csv(output_path, index=False, encoding="utf-8")
    print("Dataset disimpan ke:", output_path)

if __name__ == "__main__":
    main()

file_path = "my_food_recipes_new.csv"
df = pd.read_csv(file_path)

df.head()

def check_and_fix_columns(df):
    """
    Memeriksa & memperbaiki kolom yang salah isi (geser kolom).
    Target kolom:
    - Title
    - Ingredients
    - Steps
    - URL
    - Category
    - Title Cleaned

    Return: dataframe yang sudah diperbaiki
    """

    df = df.copy()

    def is_url(x):
        return isinstance(x, str) and x.startswith("http")

    def is_ingredients(x):
        return isinstance(x, str) and (
            "--" in x or any(k in x.lower() for k in ["ayam", "bawang", "telur", "daging"])
        )

    def is_steps(x):
        return isinstance(x, str) and any(
            k in x.lower()
            for k in ["siapkan", "rebus", "goreng", "tumis", "masak", "campur"]
        )

    def is_title(x):
        return isinstance(x, str) and len(x.split()) <= 10

    def is_category(x):
        return isinstance(x, str) and len(x.split()) <= 3

    for idx, row in df.iterrows():
        title = row["Title"]
        ingredients = row["Ingredients"]
        steps = row["Steps"]
        url = row["URL"]
        category = row["Category"]

        # === PERBAIKAN URL ===
        if not is_url(url):
            for col in ["Steps", "Ingredients", "Title"]:
                if is_url(row[col]):
                    df.at[idx, "URL"] = row[col]
                    df.at[idx, col] = ""
                    break

        # === PERBAIKAN INGREDIENTS ===
        if not is_ingredients(ingredients):
            for col in ["Steps", "Title"]:
                if is_ingredients(row[col]):
                    df.at[idx, "Ingredients"] = row[col]
                    df.at[idx, col] = ""
                    break

        # === PERBAIKAN STEPS ===
        if not is_steps(steps):
            for col in ["Ingredients", "Title"]:
                if is_steps(row[col]):
                    df.at[idx, "Steps"] = row[col]
                    df.at[idx, col] = ""
                    break

        # === PERBAIKAN TITLE ===
        if not is_title(title):
            for col in ["Ingredients", "Steps"]:
                if is_title(row[col]):
                    df.at[idx, "Title"] = row[col]
                    df.at[idx, col] = ""
                    break

        # === PERBAIKAN CATEGORY ===
        if not is_category(category):
            for col in ["Title", "Ingredients"]:
                if is_category(row[col]):
                    df.at[idx, "Category"] = row[col]
                    df.at[idx, col] = ""
                    break

    return df

def build_ingredient_vocabulary(df, column="Ingredients Cleaned"):
    """
    Membuat vocabulary kata bahan + frekuensinya
    """
    vocab = Counter()

    for text in df[column].dropna():
        for word in text.split(","):
            for token in word.strip().split():
                if len(token) > 2:   # buang kata super pendek
                    vocab[token] += 1

    return vocab

def correct_typo(word, vocab, min_freq=30, similarity=0.85):
    """
    Koreksi typo berbasis:
    - kemiripan huruf
    - frekuensi kemunculan
    """

    # jika kata sudah umum, jangan diubah
    if vocab.get(word, 0) >= min_freq:
        return word

    # cari kandidat mirip
    candidates = get_close_matches(
        word,
        vocab.keys(),
        n=3,
        cutoff=similarity
    )

    if not candidates:
        return word

    # pilih kandidat dengan frekuensi tertinggi
    best = max(candidates, key=lambda w: vocab[w])

    # pastikan kandidat memang lebih umum
    if vocab[best] > vocab.get(word, 0) * 3:
        return best

    return word

def preprocess_ingredients(text, vocab=None):
    """
    FINAL – KUAT + TYPO CORRECTION
    """

    if pd.isna(text):
        return ""

    ingredients_list = [x.strip() for x in text.split("--")]
    cleaned = []

    units = (
        r'gram|gr|g|kg|ml|l|liter|sdm|sdt|sendok makan|sendok teh|bh|'
        r'butir|buah|siung|lembar|batang|papan|bungkus|sachet'
    )

    brands = (
        r'rose brand|indofood|segitiga biru|kobe|sasa|bango|segitiga|biru|'
        r'filma|sania|tropicana slim|blue band|anchor|kraft|'
        r'masako|royco|ajinomoto'
    )

    instructions = (
        r'untuk .*|secukupnya|'
        r'goreng|digoreng|menggoreng|'
        r'rebus|direbus|'
        r'kukus|dikukus|'
        r'iris|diiris|'
        r'cincang|dicincang|'
        r'halus|dihaluskan|'
        r'potong|dipotong|'
        r'pipihkan|memarkan|'
        r'kupas|dikupas|'
        r'buang.*|ambil.*'
    )

    for item in ingredients_list:
        text = item.lower().strip()

        if ":" in text:
            continue

        if "," in text:
            text = text.split(",")[0].strip()

        if re.match(r'^(bahan|bumbu)\b', text) and len(text.split()) <= 3:
            continue

        text = re.sub(r'\(.*?\)', '', text)
        text = re.sub(r'^[0-9¼½¾⅓⅔/.\s]+', '', text)
        text = re.sub(rf'^(?:{units})\s+', '', text)
        text = re.sub(rf'\b(?:{brands})\b', '', text)
        text = re.sub(rf'\b(?:{instructions})\b', '', text)

        text = re.sub(r'[^a-z\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()

        if not text:
            continue

        # =========================
        # TYPO CORRECTION PER KATA
        # =========================
        words = text.split()
        if vocab:
            words = [correct_typo(w, vocab) for w in words]

        final_text = " ".join(words)

        if final_text and final_text not in cleaned:
            cleaned.append(final_text)

    return ", ".join(cleaned)

#main
df_fixed = check_and_fix_columns(df)

# 1. build vocab dulu
vocab = build_ingredient_vocabulary(df_fixed, "Ingredients Cleaned")

# 2. preprocess ulang dengan typo correction
df_fixed["Ingredients Cleaned"] = df_fixed["Ingredients"].apply(
    lambda x: preprocess_ingredients(x, vocab)
)

df_fixed.head()

print(df_fixed["Ingredients Cleaned"].isnull().sum())

empty_strings_count = (df_fixed['Ingredients Cleaned'] == '').sum()
print(f"Jumlah baris dengan 'Ingredients Cleaned' string kosong: {empty_strings_count}")

df_fixed = df_fixed[df_fixed['Ingredients Cleaned'] != '']
df_fixed = df_fixed.reset_index(drop=True)

print("DataFrame setelah menghapus baris dengan 'Ingredients Cleaned' kosong:")
display(df_fixed.head())

empty_strings_count_after_drop = (df_fixed['Ingredients Cleaned'] == '').sum()
print(f"Jumlah baris dengan 'Ingredients Cleaned' string kosong setelah dihapus: {empty_strings_count_after_drop}")

# coba dataframe ke CSV
df_fixed.to_csv("output_resep_cleaned.csv", index=False, encoding="utf-8-sig")

def extract_tfidf_features(
    df,
    text_column="Ingredients Cleaned",
    use_bigram=False,
    min_df=2,
    max_df=0.9
):
    """
    Ekstraksi fitur TF-IDF dari kolom teks bahan.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe input
    text_column : str
        Nama kolom teks (default: 'Ingredients Cleaned')
    use_bigram : bool
        False = unigram, True = unigram + bigram
    min_df : int
        Minimum document frequency
    max_df : float
        Maximum document frequency

    Returns
    -------
    X_tfidf : sparse matrix
        Matriks fitur TF-IDF
    vectorizer : TfidfVectorizer
        Objek vectorizer yang sudah di-fit
    """

    # validasi kolom
    if text_column not in df.columns:
        raise ValueError(f"Kolom '{text_column}' tidak ditemukan")

    # pastikan string & tidak NaN
    texts = df[text_column].fillna("").astype(str)

    # set ngram
    ngram_range = (1, 2) if use_bigram else (1, 1)

    # TF-IDF Vectorizer
    vectorizer = TfidfVectorizer(
        ngram_range=ngram_range,
        min_df=min_df,
        max_df=max_df
    )

    X_tfidf = vectorizer.fit_transform(texts)

    return X_tfidf, vectorizer

# unigram
X_tfidf, tfidf_vectorizer = extract_tfidf_features(df_fixed)

print(X_tfidf.shape)

# unigram + bigram
X_tfidf_ngram, tfidf_vectorizer_ngram = extract_tfidf_features(
    df_fixed,
    use_bigram=True
)

def cluster_recipes_kmeans(
    df,
    X_tfidf,
    vectorizer,
    n_clusters=5,
    random_state=42
):
    """
    Melakukan clustering resep menggunakan K-Means berbasis TF-IDF.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe resep
    X_tfidf : sparse matrix
        Matriks fitur TF-IDF
    vectorizer : TfidfVectorizer
        Vectorizer hasil fit TF-IDF
    n_clusters : int
        Jumlah cluster
    random_state : int
        Seed random

    Returns
    -------
    df_clustered : pandas.DataFrame
        Dataframe dengan kolom 'cluster'
    kmeans : KMeans
        Model K-Means terlatih
    """

    # Fit K-Means
    kmeans = KMeans(
        n_clusters=n_clusters,
        random_state=random_state,
        n_init=10
    )

    cluster_labels = kmeans.fit_predict(X_tfidf)

    # Salin dataframe agar aman
    df_clustered = df.copy()
    df_clustered["cluster"] = cluster_labels

    return df_clustered, kmeans

def interpret_clusters(
    df_clustered,
    X_tfidf,
    vectorizer,
    top_n=10
):
    """
    Menampilkan bahan dominan pada tiap cluster
    untuk interpretasi cluster.

    Returns: dictionary {cluster_id: [top terms]}
    """

    feature_names = vectorizer.get_feature_names_out()
    clusters = {}

    for cluster_id in sorted(df_clustered["cluster"].unique()):
        idx = df_clustered[df_clustered["cluster"] == cluster_id].index

        mean_tfidf = X_tfidf[idx].mean(axis=0)
        mean_tfidf = np.asarray(mean_tfidf).flatten()

        top_indices = mean_tfidf.argsort()[::-1][:top_n]
        top_terms = [feature_names[i] for i in top_indices]

        clusters[cluster_id] = top_terms

    return clusters

# === TF-IDF ===
X_tfidf, tfidf_vectorizer = extract_tfidf_features(df_fixed)

# === K-Means Clustering ===
df_clustered, kmeans_model = cluster_recipes_kmeans(
    df_fixed,
    X_tfidf,
    tfidf_vectorizer,
    n_clusters=5
)

# === Interpretasi Cluster ===
cluster_terms = interpret_clusters(
    df_clustered,
    X_tfidf,
    tfidf_vectorizer,
    top_n=10
)

for cid, terms in cluster_terms.items():
    print(f"Cluster {cid}: {terms}")

import re

def preprocess_user_input(user_input_text):
    """
    Preprocessing kolom ingredients dari input user sesuai permintaan:
    - split bahan berdasarkan ','
    - lowercase
    - hapus angka di depan
    - hapus satuan di depan
    - output: 'tahu, tempe, wortel'
    """

    if not isinstance(user_input_text, str):
        return ""

    # =========================
    # SATUAN
    # =========================
    units = (
        r'gram|g|gr|kg|ml|l|liter|sdm|sdt|sendok makan|sendok teh|bh|'
        r'butir|buah|siung|lembar|batang|papan|bungkus|sachet'
    )

    # 1. Split by comma
    items_raw = [item.strip() for item in user_input_text.split(',')]
    cleaned_items = []

    for item in items_raw:
        # 2. Lowercase
        text = item.lower()

        # Hapus isi kurung (opsional, mengikuti pola preprocess_ingredients)
        text = re.sub(r'\(.*?\)', '', text)

        # 3. Remove leading numbers
        text = re.sub(r'^[0-9¼½¾⅓⅔/.\s]+', '', text).strip()

        # 3. Remove leading units
        text = re.sub(rf'^(?:{units})\s*', '', text).strip()

        # Basic cleaning (remove non-alphanumeric except spaces, then consolidate spaces)
        text = re.sub(r'[^a-z\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()

        if text and text not in cleaned_items: # Avoid duplicates
            cleaned_items.append(text)

    return ", ".join(cleaned_items)

"""Tambahan"""

def ingredient_match_score(user_cleaned, recipe_cleaned):
    user_items = user_cleaned.split(", ")
    recipe_items = recipe_cleaned.split(", ")

    match = 0
    for u in user_items:
        for r in recipe_items:
            if u in r or r in u:
                match += 1
                break

    return match / len(user_items)

def recommend_recipes_knn(
    user_input,
    df,
    X_tfidf,
    vectorizer,
    kmeans_model=None,
    top_k=5,
    min_coverage=0.5   # ← minimal 50% bahan cocok
):
    user_cleaned = preprocess_user_input(user_input)
    user_vector = vectorizer.transform([user_cleaned])

    # cluster (opsional)
    if kmeans_model is not None:
        user_cluster = kmeans_model.predict(user_vector)[0]
        candidate_idx = df[df["cluster"] == user_cluster].index
        X_candidates = X_tfidf[candidate_idx]
    else:
        candidate_idx = df.index
        X_candidates = X_tfidf

    knn = NearestNeighbors(n_neighbors=len(candidate_idx), metric="cosine")
    knn.fit(X_candidates)

    distances, indices = knn.kneighbors(user_vector)

    results = []

    for dist, idx in zip(distances[0], indices[0]):
        real_idx = candidate_idx[idx]
        recipe = df.loc[real_idx]

        # ===== TAMBAHAN BARU =====
        coverage = ingredient_match_score(
            user_cleaned,
            recipe["Ingredients Cleaned"]
        )

        if coverage < min_coverage:
            continue

        similarity = 1 - dist

        final_score = 0.5 * similarity + 0.5 * coverage

        results.append({
            "Title": recipe["Title"],
            "Ingredients": recipe["Ingredients Cleaned"],
            # "Steps": recipe["Steps"],
            # "URL": recipe["URL"],
            "similarity": similarity,
            "ingredient_match": coverage,
            "final_score": final_score
        })

    results = sorted(results, key=lambda x: x["final_score"], reverse=True)

    return pd.DataFrame(results).head(top_k)

# Input pengguna
user_input = input("Masukkan bahan (pisahkan dengan koma): ")

# Rekomendasi
recommendations = recommend_recipes_knn(
    user_input=user_input,
    df=df_clustered,          # dataframe hasil clustering
    X_tfidf=X_tfidf,
    vectorizer=tfidf_vectorizer,
    kmeans_model=kmeans_model,  # bisa None jika tidak pakai cluster
    top_k=5
)

recommendations

"""**Market Basket Analysis (MBA)**"""

from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import warnings
warnings.filterwarnings("ignore")

# Ambil kolom ingredients cleaned
ingredients_series = df_fixed["Ingredients Cleaned"].dropna()

print("Jumlah resep yang dipakai:", len(ingredients_series))
ingredients_series.head()

# Ubah tiap resep jadi list bahan
transactions = ingredients_series.apply(
    lambda x: [item.strip() for item in x.split(",") if item.strip()]
).tolist()

print("Contoh 3 transaksi pertama:")
transactions[:3]

from collections import Counter

ingredient_counts = Counter()

for items in transactions:
    ingredient_counts.update(items)

# Ambil bahan yang muncul minimal 50 kali
frequent_ingredients = {
    ing for ing, cnt in ingredient_counts.items() if cnt >= 10
}

# Filter transaksi
transactions = [
    [ing for ing in items if ing in frequent_ingredients]
    for items in transactions
]

# BUANG transaksi yang kurang dari 2 bahan
transactions = [t for t in transactions if len(t) >= 2]

# One-hot encoding

# 1. One-hot encoding (ada / tidak ada)
te = TransactionEncoder()
te_array = te.fit(transactions).transform(transactions)

# 2. Ubah ke DataFrame
df_basket = pd.DataFrame(te_array, columns=te.columns_)

# === TAMBAHAN DI SINI (BIAR SEMUA BAHAN KELIHATAN) ===
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)

# 3. Tampilkan hasil
df_basket.head(20)

total_true = df_basket.values.sum()
total_cells = df_basket.size
total_false = total_cells - total_true

print("Jumlah TRUE  :", total_true)
print("Jumlah FALSE :", total_false)
print("Total sel    :", total_cells)

from mlxtend.frequent_patterns import apriori

frequent_itemsets = apriori(
    df_basket,
    min_support=0.02,
    use_colnames=True,
    max_len=2
)

print("Jumlah frequent itemsets:", len(frequent_itemsets))
frequent_itemsets.head()

rules = association_rules(
    frequent_itemsets,
    metric="confidence",
    min_threshold=0.4
)

rules.head()

rules_sorted = rules.sort_values(by="lift", ascending=False)

rules_sorted[
    ["antecedents", "consequents", "support", "confidence", "lift"]
].head(10)

# Simpan model dan data
import pickle

with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf_vectorizer, f)

with open('kmeans_model.pkl', 'wb') as f:
    pickle.dump(kmeans_model, f)

df_clustered.to_pickle('df_clustered.pkl')

print("Model dan data berhasil disimpan!")

if __name__ == "__main__":
    main()